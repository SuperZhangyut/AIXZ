library(VGAM)
library(mvtnorm)
library(dr)
library(clime)
library(CompQuadForm)
library(psych)
library(energy)
library(glmnet)
## source("../../DT_SIR.R")
source("../../SIR_LASSO.R")




wine <- read.csv("winequality-white.csv",sep=";")

p <- dim(wine)[2]-1
n <- dim(wine)[1]

X <- wine[,1:11]
Y <- wine[,12]
X <- as.matrix(X)
Y <- as.matrix(Y)

y <- matrix( 0, nrow=n, ncol=length( unique( Y ) ) )
for(i in 1:n )
  y[ i, Y[i]-2 ] <- 1
colnames( y ) <- c("s3","s4","s5","s6","s7","s8","s9")

X.mean <- array( apply( X, 2, mean ), c(1,p) )%x% array(1, c(n, 1) )
X.std <- array( sqrt( apply( X,2,var)), c(1,p))%x%array(1, c(n, 1) )

X <- (X-X.mean)/X.std

no.dim <- 2

H <- 16
m <- n/H
H1 <- H
m1 <- m
H2 <- H
m2 <- m



postscript("wine_pca.eps",horizontal=FALSE)
## PCA
wine.pca <- princomp(X)
comp.pca <- data.frame( wine.pca$scores[,1], wine.pca$scores[,2],  Y, y )

colnames( comp.pca ) <- c("C1","C2","Y","s3","s4","s5","s6","s7","s8","s9")
pca.fit <- vglm( cbind(s3,s4,s5,s6,s7,s8,s9)~ C1 + C2, data=comp.pca, fam=multinomial(parallel=TRUE~C1+C2-1) )


p1 <- ggplot( comp.pca, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("PCA")
dev.off()

postscript("wine_spca.eps",horizontal=FALSE)
## Sparse PCA
wine.spca <- nsprcomp(X)
wine.spca$score <- X%*% wine.spca$rotation
comp.spca <- data.frame( wine.spca$score[,1], wine.spca$score[,2],  Y )

colnames( comp.spca ) <- c("C1","C2","Y")
p1 <- ggplot( comp.spca, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("Sparse PCA")
dev.off()




## ## DT-SIR
postscript("wine_dt_sir.eps",horizontal=FALSE)
tmp <- cov(X)
SigmaX.inv <- solve( tmp*( abs(tmp)> (  log(p)/sqrt(n))) )
dt.sir.beta <- SSIR(X, SigmaX.inv, Y, p, m1, m2, n, H1, H2, no.dim, categorical=TRUE )$beta
dt.sir.beta[,1] <- dt.sir.beta[,1]/sqrt( sum( dt.sir.beta[,1]^2 ) )
dt.sir.beta[,2] <- dt.sir.beta[,2]/sqrt( sum( dt.sir.beta[,2]^2 ) )

comp.dt.sir <- X %*% dt.sir.beta 

comp.dt.sir.data <- data.frame( comp.dt.sir[,1], comp.dt.sir[,2], Y)

colnames( comp.dt.sir.data ) <- c("C1","C2","Y")
p1 <- ggplot( comp.dt.sir.data, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("DT SIR")

dev.off()



## ## SIR LASSO
postscript("wine_lasso_sir.eps",horizontal=FALSE)
sir.lasso.beta <-  SIR_LASSO.A2( X, Y, p, n, H, m, no.dim, solution.path=FALSE, categorical=TRUE, nfolds=10)
sir.lasso.beta[,1] <- sir.lasso.beta[,1]/sqrt( sum( sir.lasso.beta[,1]^2 ) )
sir.lasso.beta[,2] <- sir.lasso.beta[,2]/sqrt( sum( sir.lasso.beta[,2]^2 ) )

comp.sir.lasso <- X %*% sir.lasso.beta 

comp.sir.lasso.data <- data.frame( comp.sir.lasso[,1], comp.sir.lasso[,2], Y)

colnames( comp.sir.lasso.data ) <- c("C1","C2","Y")
p1 <- ggplot( comp.sir.lasso.data, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("SIR LASSO")

dev.off()



## LASSO
## ## LASSO
comp.lasso <- array(0, c(n,2) )
lars.fit.cv <- cv.glmnet( X, cbind( (Y==1), (Y!=1) ), family="binomial" )
## two ways of choosing the threshold. 1. choose the one with the smallest cvm; 2. choose the one with a significant smallest cvm. Namely, choose the one such that the lower bound is higher than the upper bound of the one with the smallest cvm.
ind <- which( lars.fit.cv$cvm==min(lars.fit.cv$cvm) )

lambda <- lars.fit.cv$lambda[ max( ( lars.fit.cv$cvlo[1:ind] >= lars.fit.cv$cvup[ind] ) * c(1:ind) ) ]
lambda <- lars.fit.cv$lambda[ind]
lars.fit <- glmnet(X, Y, lambda=lambda)
lasso.beta <- lars.fit$beta
comp.lasso[,1] <- (X %*% lasso.beta)[,1]
                                             

lars.fit.cv <- cv.glmnet( X, cbind( (Y==2), (Y!=2) ), family="binomial" )
## two ways of choosing the threshold. 1. choose the one with the smallest cvm; 2. choose the one with a significant smallest cvm. Namely, choose the one such that the lower bound is higher than the upper bound of the one with the smallest cvm.
ind <- which( lars.fit.cv$cvm==min(lars.fit.cv$cvm) )

lambda <- lars.fit.cv$lambda[ max( ( lars.fit.cv$cvlo[1:ind] >= lars.fit.cv$cvup[ind] ) * c(1:ind) ) ]
lambda <- lars.fit.cv$lambda[ind]
lars.fit <- glmnet(X, Y, lambda=lambda)
lasso.beta <- lars.fit$beta
comp.lasso[,2] <- (X %*% lasso.beta)[,1]

postscript("wine_lasso.eps",horizontal=FALSE)

comp.lasso.data <- data.frame( comp.lasso[,1], comp.lasso[,2], Y)
colnames( comp.lasso.data ) <- c("C1","C2","Y")
p1 <- ggplot( comp.lasso.data, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("LASSO")

dev.off()


## Li Lexin's method
postscript("sdr.eps",horizontal=FALSE)
sdr <- comp.sdr.sir(X, Y, no.dim, floor(sqrt(n)) )
comp.sdr <- X%*%sdr$beta.sdr
comp.sdr.data <- data.frame( comp.sdr[,1], comp.sdr[,2], Y)
colnames( comp.sdr.data ) <- c("C1","C2", "Y")
p1 <- ggplot( comp.sdr.data, aes( x=C1, y=C2 ) )
p1 + geom_point( aes(color=factor( Y) )) + theme( legend.position="none" ) + ggtitle("SDR")

dev.off()



















#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################


X.full <- X
Y.full <- Y

## Use leave-one-out cross validation for prediction
pca.pred <- array( 0, length(Y) )
sir.lasso.pred <- pca.pred
dt.sir.pred <- pca.pred
lasso.pred <- pca.pred

for( i in 1:length(Y) )
  {
    Y <- as.matrix( Y.full[ setdiff( c(1:length(Y.full)), i)  ], nrow=length(Y.full)-1, ncol=1)
    X <- X.full[ setdiff( c(1:length(Y.full)), i ), ]
    p <- dim(X)[2]
    n <- dim(X)[1]
    
    ## PCA
    wine.pca <- princomp(X)
    pca.data <- data.frame(Y, wine.pca$scores )
    ##    base.logit.pca <- vglm( cbind(s3,s4,s5,s6,s7,s8) ~ Comp.1 + Comp.2  , data=pca.data, fam=multinomial(parallel=TRUE) )
    base.lm.pca <- lm( Y~Comp.1 + Comp.2, data=pca.data )
    ## summary(base.logit.pca)
    data.pred.pca <- data.frame( X.full[i,] %*% wine.pca$loadings[,1:2] )
    pca.pred[i] <- predict( base.lm.pca, newdata=data.pred.pca )
    
    
    ## ## SIR LASSO
    sir.lasso.beta <-  SIR_LASSO.A2( X, Y, p, n, H, m, no.dim, solution.path=FALSE, categorical=TRUE, nfolds=10)
    sir.lasso.beta[,1] <- sir.lasso.beta[,1]/sqrt( sum( sir.lasso.beta[,1]^2 ) )
    sir.lasso.beta[,2] <- sir.lasso.beta[,2]/sqrt( sum( sir.lasso.beta[,2]^2 ) )

    
    comp.sir.lasso <- X %*% sir.lasso.beta 
    sir.lasso.data <- data.frame( Y, comp.sir.lasso )
    lm.sir.lasso <- lm(Y~X1 +X2, data=sir.lasso.data)
    data.pred.sir.lasso <- data.frame( X.full[i,] %*% sir.lasso.beta )
    sir.lasso.pred[i] <- predict( lm.sir.lasso, newdata= data.pred.sir.lasso )
    


    ## ## DT-SIR
    tmp <- cov(X)
    SigmaX.inv <- solve( tmp*( abs(tmp)> (  log(p)/sqrt(n))) )
    dt.sir.beta <- SSIR(X, SigmaX.inv, Y, p, m1, m2, n, H1, H2, no.dim, 0 )
    comp.dt.sir <- X%*%dt.sir.beta$beta
    
    dt.sir.data <- data.frame(Y, comp.dt.sir )
    lm.dt.sir <- lm(Y~X1 + X2, data=dt.sir.data)
    data.pred.dt.sir <- data.frame( X.full[i,] %*% dt.sir.beta$beta )
    dt.sir.pred[i] <- predict( lm.dt.sir, newdata= data.pred.dt.sir )
    
    
    
    ## ## LASSO
    lars.fit.cv <- cv.glmnet( X, Y )
    ind <- which( lars.fit.cv$cvm==min(lars.fit.cv$cvm) )
    if(ind==1)
      ind=2
    ## two ways of choosing the threshold. 1. choose the one with the smallest cvm; 2. choose the one with a significant smallest cvm. Namely, choose the one such that the lower bound is higher than the upper bound of the one with the smallest cvm.
    ## lambda <- lars.fit.cv$lambda[ max( ( lars.fit.cv$cvlo[1:ind] >= lars.fit.cv$cvup[ind] ) * c(1:ind) ) ]
    lambda <- lars.fit.cv$lambda[ind]
    lars.fit <- glmnet(X, Y, lambda=lambda)
    lasso.beta <- lars.fit$beta
    comp.lasso <- X.full[i,]%*%lasso.beta
    lasso.pred[i] <- mean(Y) + comp.lasso[1]

    print(i)
    
  }
